"""
DiT (Diffusion Transformer) æ¨¡å‹å®ç°

DiT æ˜¯ä¸€ç§åŸºäº Transformer çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå›¾åƒç”Ÿæˆä»»åŠ¡ã€‚
ä¸ä¼ ç»Ÿçš„ UNet æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒDiT ä½¿ç”¨ Transformer æ¶æ„å¤„ç†å›¾åƒ patchesã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
1. å°†å›¾åƒåˆ‡åˆ†æˆ patchesï¼ˆç±»ä¼¼ ViTï¼‰
2. ä½¿ç”¨ Transformer å¤„ç† patch åºåˆ—
3. æ¡ä»¶ä¿¡æ¯ï¼ˆæ—¶é—´æ­¥ t å’Œæ ‡ç­¾ yï¼‰é€šè¿‡ AdaLNï¼ˆè‡ªé€‚åº”å±‚å½’ä¸€åŒ–ï¼‰æ³¨å…¥
4. æœ€åå°† patches é‡ç»„å›å›¾åƒ
"""

from torch import nn 
import torch 
from time_emb import TimeEmbedding
from dit_block import DiTBlock
from config import T 

class DiT(nn.Module):
    """
    Diffusion Transformer æ¨¡å‹
    
    å‚æ•°è¯´æ˜ï¼š
        img_size: å›¾åƒå¤§å°ï¼ˆä¾‹å¦‚ 28 è¡¨ç¤º 28Ã—28ï¼‰
        patch_size: æ¯ä¸ª patch çš„å¤§å°ï¼ˆä¾‹å¦‚ 4 è¡¨ç¤º 4Ã—4ï¼‰
        channel: å›¾åƒé€šé“æ•°ï¼ˆç°åº¦å›¾ä¸º 1ï¼ŒRGB ä¸º 3ï¼‰
        emb_size: embedding ç»´åº¦ï¼ˆTransformer çš„éšè—å±‚ç»´åº¦ï¼‰
        label_num: åˆ†ç±»æ ‡ç­¾æ•°é‡ï¼ˆMNIST ä¸º 10ï¼‰
        dit_num: DiT Block çš„å †å å±‚æ•°
        head: Multi-head Attention çš„å¤´æ•°
    """
    def __init__(self, img_size, patch_size, channel, emb_size, label_num, dit_num, head):
        super().__init__()
        
        # ===== åŸºæœ¬å‚æ•° =====
        self.patch_size = patch_size
        self.patch_count = img_size // self.patch_size  # æ¯ä¸ªç»´åº¦æœ‰å¤šå°‘ä¸ª patchï¼ˆä¾‹å¦‚ 28/4=7ï¼‰
        self.channel = channel
        
        # ===== Patchify å±‚ï¼šå°†å›¾åƒåˆ‡åˆ†æˆ patches =====
        # ä½¿ç”¨å·ç§¯å°†å›¾åƒåˆ‡åˆ†æˆä¸é‡å çš„ patches
        # è¾“å…¥: (batch, channel, img_size, img_size)
        # è¾“å‡º: (batch, channel*patch_size^2, patch_count, patch_count)
        # ä¾‹å¦‚ï¼š(batch, 1, 28, 28) â†’ (batch, 16, 7, 7)
        self.conv = nn.Conv2d(
            in_channels=channel,
            out_channels=channel * patch_size**2,  # æ¯ä¸ª patch å±•å¹³åçš„ç»´åº¦
            kernel_size=patch_size,
            padding=0,
            stride=patch_size  # stride=patch_size ä¿è¯ patches ä¸é‡å 
        ) 
        
        # Patch Embeddingï¼šå°†å±•å¹³çš„ patch æ˜ å°„åˆ° embedding ç©ºé—´
        # (channel*patch_size^2) â†’ emb_size
        # ä¾‹å¦‚ï¼š16 â†’ 64
        self.patch_emb = nn.Linear(
            in_features=channel * patch_size**2,
            out_features=emb_size
        ) 
        
        # Patch ä½ç½®ç¼–ç ï¼šä¸ºæ¯ä¸ª patch ä½ç½®æ·»åŠ å¯å­¦ä¹ çš„ä½ç½®ä¿¡æ¯
        # shape: (1, patch_count^2, emb_size)
        # ä¾‹å¦‚ï¼š(1, 49, 64) - 49ä¸ªpatchçš„ä½ç½®ç¼–ç 
        self.patch_pos_emb = nn.Parameter(torch.rand(1, self.patch_count**2, emb_size))
        
        # ===== Time Embeddingï¼šå°†æ—¶é—´æ­¥ t ç¼–ç ä¸ºå‘é‡ =====
        # æ—¶é—´æ­¥ t âˆˆ [0, T-1] éœ€è¦è½¬æ¢ä¸ºé«˜ç»´å‘é‡æ¥æŒ‡å¯¼å»å™ª
        # ä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç  + MLP
        self.time_emb = nn.Sequential(
            TimeEmbedding(emb_size),      # æ—¶é—´æ­¥çš„æ­£å¼¦ä½ç½®ç¼–ç 
            nn.Linear(emb_size, emb_size),
            nn.ReLU(),
            nn.Linear(emb_size, emb_size)  # æœ€ç»ˆè¾“å‡º (batch, emb_size)
        )

        # ===== Label Embeddingï¼šå°†ç±»åˆ«æ ‡ç­¾ç¼–ç ä¸ºå‘é‡ =====
        # ç”¨äºæ¡ä»¶ç”Ÿæˆï¼šæŒ‡å®šç”Ÿæˆå“ªä¸ªç±»åˆ«çš„å›¾åƒ
        # ä¾‹å¦‚ï¼šy=3 â†’ embedding vector (emb_size,)
        self.label_emb = nn.Embedding(
            num_embeddings=label_num,  # æ€»å…±æœ‰å¤šå°‘ä¸ªç±»åˆ«
            embedding_dim=emb_size     # embedding ç»´åº¦
        )
        
        # ===== DiT Blocksï¼šæ ¸å¿ƒ Transformer å±‚ =====
        # å †å å¤šä¸ª DiT Blockï¼Œæ¯ä¸ª Block åŒ…å«ï¼š
        # - Self-Attentionï¼ˆå¤„ç† patch ä¹‹é—´çš„å…³ç³»ï¼‰
        # - AdaLNï¼ˆè‡ªé€‚åº”å±‚å½’ä¸€åŒ–ï¼Œèå…¥æ¡ä»¶ä¿¡æ¯ï¼‰
        # - MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰
        self.dits = nn.ModuleList()
        for _ in range(dit_num):
            self.dits.append(DiTBlock(emb_size, head))
        
        # ===== Layer Normï¼šæœ€åçš„å½’ä¸€åŒ–å±‚ =====
        self.ln = nn.LayerNorm(emb_size)
        
        # ===== Linear æŠ•å½±ï¼šå°† embedding æ˜ å°„å› patch =====
        # emb_size â†’ channel*patch_size^2
        # ä¾‹å¦‚ï¼š64 â†’ 16ï¼ˆè¿˜åŸå›åŸå§‹ patch ç»´åº¦ï¼‰
        self.linear = nn.Linear(emb_size, channel * patch_size**2)
        
    def forward(self, x, t, y):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
            x: å¸¦å™ªéŸ³çš„å›¾åƒï¼Œshape=(batch, channel, height, width)
               ä¾‹å¦‚ï¼š(batch, 1, 28, 28)
            t: æ—¶é—´æ­¥ï¼Œshape=(batch,)
               ä¾‹å¦‚ï¼š[999, 500, 250, ...] - æ¯ä¸ªæ ·æœ¬çš„æ‰©æ•£æ—¶é—´æ­¥
            y: ç±»åˆ«æ ‡ç­¾ï¼Œshape=(batch,)
               ä¾‹å¦‚ï¼š[3, 7, 1, ...] - æŒ‡å®šç”Ÿæˆçš„æ•°å­—ç±»åˆ«
               
        è¿”å›ï¼š
            é¢„æµ‹çš„å™ªéŸ³ï¼ˆæˆ–å»å™ªåçš„å›¾åƒï¼‰ï¼Œshape=(batch, channel, height, width)
        """
        
        # ===== æ­¥éª¤1ï¼šæ¡ä»¶ç¼–ç ï¼ˆCondition Embeddingï¼‰=====
        # å°†æ ‡ç­¾å’Œæ—¶é—´æ­¥éƒ½ç¼–ç ä¸ºå‘é‡ï¼Œç„¶åç›¸åŠ ä½œä¸ºæ¡ä»¶ä¿¡å·
        y_emb = self.label_emb(y)  # (batch, emb_size) - æ ‡ç­¾ embedding
        t_emb = self.time_emb(t)   # (batch, emb_size) - æ—¶é—´ embedding
        
        # æ¡ä»¶å‘é‡ = æ ‡ç­¾ä¿¡æ¯ + æ—¶é—´ä¿¡æ¯
        # è¿™ä¸ªå‘é‡ä¼šé€šè¿‡ AdaLN æ³¨å…¥åˆ°æ¯ä¸ª DiT Block ä¸­
        cond = y_emb + t_emb  # (batch, emb_size)
        
        # ===== æ­¥éª¤2ï¼šPatchify - å°†å›¾åƒåˆ‡åˆ†æˆ patches =====
        # 2.1 å·ç§¯åˆ‡åˆ†
        x = self.conv(x)  # (batch, channel*patch_size^2, patch_count, patch_count)
                          # ä¾‹å¦‚ï¼š(batch, 16, 7, 7)
        
        # 2.2 è°ƒæ•´ç»´åº¦é¡ºåºï¼Œå‡†å¤‡åºåˆ—åŒ–
        x = x.permute(0, 2, 3, 1)  # (batch, patch_count, patch_count, channel*patch_size^2)
                                    # ä¾‹å¦‚ï¼š(batch, 7, 7, 16)
        
        # 2.3 å±•å¹³æˆåºåˆ—
        x = x.view(x.size(0), self.patch_count * self.patch_count, x.size(3))
        # (batch, patch_count^2, channel*patch_size^2)
        # ä¾‹å¦‚ï¼š(batch, 49, 16) - 49ä¸ªpatchï¼Œæ¯ä¸ª16ç»´
        
        # ===== æ­¥éª¤3ï¼šPatch Embedding =====
        x = self.patch_emb(x)  # (batch, patch_count^2, emb_size)
                               # ä¾‹å¦‚ï¼š(batch, 49, 64)
        
        # ===== æ­¥éª¤4ï¼šæ·»åŠ ä½ç½®ç¼–ç  =====
        # è®©æ¨¡å‹çŸ¥é“æ¯ä¸ª patch åœ¨å›¾åƒä¸­çš„ä½ç½®
        x = x + self.patch_pos_emb  # (batch, patch_count^2, emb_size)
        
        # ===== æ­¥éª¤5ï¼šDiT Blocks å¤„ç† =====
        # é€šè¿‡å¤šå±‚ Transformer Block å¤„ç† patch åºåˆ—
        # æ¯ä¸ª Block éƒ½ä¼šåˆ©ç”¨ cond æ¡ä»¶ä¿¡æ¯
        for dit in self.dits:
            x = dit(x, cond)  # x ä¿æŒ (batch, patch_count^2, emb_size)
        
        # ===== æ­¥éª¤6ï¼šLayer Norm =====
        x = self.ln(x)  # (batch, patch_count^2, emb_size)
        
        # ===== æ­¥éª¤7ï¼šæŠ•å½±å› Patch ç»´åº¦ =====
        x = self.linear(x)  # (batch, patch_count^2, channel*patch_size^2)
                            # ä¾‹å¦‚ï¼š(batch, 49, 16)
        
        # ===== æ­¥éª¤8ï¼šUn-Patchify - å°† patches é‡ç»„å›å›¾åƒ =====
        # è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„ reshape è¿‡ç¨‹ï¼Œéœ€è¦ä»”ç»†ç†è§£ç»´åº¦å˜æ¢
        
        # 8.1 é‡å¡‘ä¸º 6D tensor
        # å°†æ¯ä¸ª patch çš„ (channel*patch_size^2) æ‹†åˆ†æˆ (channel, patch_size, patch_size)
        x = x.view(
            x.size(0),           # batch
            self.patch_count,    # å‚ç›´æ–¹å‘çš„ patch æ•°é‡
            self.patch_count,    # æ°´å¹³æ–¹å‘çš„ patch æ•°é‡
            self.channel,        # é€šé“æ•°
            self.patch_size,     # patch é«˜åº¦
            self.patch_size      # patch å®½åº¦
        )
        # shape: (batch, patch_count(H), patch_count(W), channel, patch_size(H), patch_size(W))
        # ä¾‹å¦‚ï¼š(batch, 7, 7, 1, 4, 4)
        
        # 8.2 è°ƒæ•´ç»´åº¦é¡ºåºï¼šå°† channel ç§»åˆ°å‰é¢
        x = x.permute(0, 3, 1, 2, 4, 5)
        # (batch, channel, patch_count(H), patch_count(W), patch_size(H), patch_size(W))
        # ä¾‹å¦‚ï¼š(batch, 1, 7, 7, 4, 4)
        
        # 8.3 å†æ¬¡è°ƒæ•´ï¼šå°†åŒä¸€è¡Œçš„ patches æ’åˆ—åœ¨ä¸€èµ·
        x = x.permute(0, 1, 2, 4, 3, 5)
        # (batch, channel, patch_count(H), patch_size(H), patch_count(W), patch_size(W))
        # ä¾‹å¦‚ï¼š(batch, 1, 7, 4, 7, 4)
        
        # 8.4 æœ€å reshapeï¼šåˆå¹¶ patch ç»´åº¦ï¼Œå¾—åˆ°å®Œæ•´å›¾åƒ
        x = x.reshape(
            x.size(0),                              # batch
            self.channel,                           # channel
            self.patch_count * self.patch_size,     # é«˜åº¦ = patchæ•° Ã— patchå¤§å°
            self.patch_count * self.patch_size      # å®½åº¦ = patchæ•° Ã— patchå¤§å°
        )
        # (batch, channel, img_size, img_size)
        # ä¾‹å¦‚ï¼š(batch, 1, 28, 28) âœ… æ¢å¤åŸå§‹å›¾åƒå¤§å°ï¼
        
        return x  # è¿”å›é¢„æµ‹çš„å™ªéŸ³æˆ–å»å™ªå›¾åƒ
    
if __name__ == '__main__':
    # ===== æµ‹è¯•ä»£ç  =====
    print("ğŸ§ª æµ‹è¯• DiT æ¨¡å‹")
    
    # åˆ›å»º DiT æ¨¡å‹
    dit = DiT(
        img_size=28,      # MNIST å›¾åƒå¤§å°
        patch_size=4,     # 4Ã—4 çš„ patch
        channel=1,        # ç°åº¦å›¾
        emb_size=64,      # embedding ç»´åº¦
        label_num=10,     # 10 ä¸ªæ•°å­—ç±»åˆ«
        dit_num=3,        # 3 å±‚ DiT Block
        head=4            # 4 ä¸ªæ³¨æ„åŠ›å¤´
    )
    
    # åˆ›å»ºéšæœºè¾“å…¥
    x = torch.rand(5, 1, 28, 28)  # 5 å¼ å›¾åƒ
    t = torch.randint(0, T, (5,))  # 5 ä¸ªéšæœºæ—¶é—´æ­¥
    y = torch.randint(0, 10, (5,))  # 5 ä¸ªéšæœºæ ‡ç­¾
    
    print(f"è¾“å…¥ x shape: {x.shape}")
    print(f"æ—¶é—´æ­¥ t: {t}")
    print(f"æ ‡ç­¾ y: {y}")
    
    # å‰å‘ä¼ æ’­
    outputs = dit(x, t, y)
    
    print(f"è¾“å‡º shape: {outputs.shape}")
    print(f"âœ… æµ‹è¯•é€šè¿‡ï¼è¾“å…¥è¾“å‡º shape ä¸€è‡´")
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in dit.parameters())
    print(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")