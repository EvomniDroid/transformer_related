"""
DiT Block - Diffusion Transformer çš„æ ¸å¿ƒæ¨¡å—

è¿™ä¸ªæ¨¡å—å®ç°äº† DiT çš„åŸºæœ¬æ„å»ºå—ï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯ AdaLNï¼ˆAdaptive Layer Normalizationï¼‰ã€‚
ä¸æ ‡å‡† Transformer ä¸åŒï¼ŒDiT Block é€šè¿‡æ¡ä»¶ä¿¡æ¯ï¼ˆæ—¶é—´æ­¥ + æ ‡ç­¾ï¼‰åŠ¨æ€è°ƒæ•´å½’ä¸€åŒ–å‚æ•°ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
1. ä½¿ç”¨æ¡ä»¶ä¿¡æ¯ç”Ÿæˆ scaleï¼ˆgammaï¼‰ã€shiftï¼ˆbetaï¼‰ã€gateï¼ˆalphaï¼‰å‚æ•°
2. é€šè¿‡ AdaLN-Zero å°†æ¡ä»¶ä¿¡æ¯æ³¨å…¥ Transformer
3. ä¿æŒæ ‡å‡† Self-Attention å’Œ FFN ç»“æ„

å…¬å¼ï¼š
  AdaLN(x, cond) = gamma(cond) * LayerNorm(x) + beta(cond)
  Output = x + alpha(cond) * Attention(AdaLN(x, cond))
"""

from torch import nn 
import torch 
import math 

class DiTBlock(nn.Module):
    """
    DiT Block with Adaptive Layer Normalization
    
    å‚æ•°ï¼š
        emb_size: embedding ç»´åº¦ï¼ˆæ¯ä¸ª patch çš„ç‰¹å¾ç»´åº¦ï¼‰
        nhead: multi-head attention çš„å¤´æ•°
    """
    def __init__(self, emb_size, nhead):
        super().__init__()
        
        self.emb_size = emb_size
        self.nhead = nhead
        
        # ===== AdaLN æ¡ä»¶å‚æ•°ç”Ÿæˆå™¨ =====
        # ä»æ¡ä»¶å‘é‡ (batch, emb_size) ç”Ÿæˆå½’ä¸€åŒ–å‚æ•°
        # æ¯ä¸ª Linear å±‚éƒ½æ˜¯ emb_size â†’ emb_size
        
        # ç¬¬ä¸€ç»„ï¼šç”¨äº Self-Attention ä¹‹å‰çš„ AdaLN
        self.gamma1 = nn.Linear(emb_size, emb_size)  # scale å‚æ•°ï¼ˆç¼©æ”¾ï¼‰
        self.beta1 = nn.Linear(emb_size, emb_size)   # shift å‚æ•°ï¼ˆå¹³ç§»ï¼‰
        self.alpha1 = nn.Linear(emb_size, emb_size)  # gate å‚æ•°ï¼ˆé—¨æ§ï¼Œæ§åˆ¶æ®‹å·®å¼ºåº¦ï¼‰
        
        # ç¬¬äºŒç»„ï¼šç”¨äº Feed-Forward ä¹‹å‰çš„ AdaLN
        self.gamma2 = nn.Linear(emb_size, emb_size)  # scale å‚æ•°
        self.beta2 = nn.Linear(emb_size, emb_size)   # shift å‚æ•°
        self.alpha2 = nn.Linear(emb_size, emb_size)  # gate å‚æ•°
        
        # ===== Layer Normalization =====
        self.ln1 = nn.LayerNorm(emb_size)  # Attention å‰çš„ LN
        self.ln2 = nn.LayerNorm(emb_size)  # FFN å‰çš„ LN
        
        # ===== Multi-Head Self-Attention =====
        # å°† emb_size æ‰©å±•åˆ° nhead * emb_sizeï¼Œç„¶åæ‹†åˆ†æˆå¤šä¸ªå¤´
        self.wq = nn.Linear(emb_size, nhead * emb_size)  # Query æŠ•å½±
        self.wk = nn.Linear(emb_size, nhead * emb_size)  # Key æŠ•å½±
        self.wv = nn.Linear(emb_size, nhead * emb_size)  # Value æŠ•å½±
        self.lv = nn.Linear(nhead * emb_size, emb_size)  # å¤šå¤´è¾“å‡ºåˆå¹¶
        
        # ===== Feed-Forward Network (MLP) =====
        # æ ‡å‡†çš„ä¸¤å±‚ MLPï¼Œä¸­é—´ç»´åº¦æ‰©å±• 4 å€
        self.ff = nn.Sequential(
            nn.Linear(emb_size, emb_size * 4),  # æ‰©å±•
            nn.ReLU(),
            nn.Linear(emb_size * 4, emb_size)   # å‹ç¼©å›åŸç»´åº¦
        )

    def forward(self, x, cond):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
            x: è¾“å…¥ patch åºåˆ—ï¼Œshape=(batch, seq_len, emb_size)
               ä¾‹å¦‚ï¼š(batch, 49, 64) - 49ä¸ªpatch
            cond: æ¡ä»¶å‘é‡ï¼ˆæ—¶é—´ + æ ‡ç­¾ï¼‰ï¼Œshape=(batch, emb_size)
                  ä¾‹å¦‚ï¼š(batch, 64)
        
        è¿”å›ï¼š
            è¾“å‡ºåºåˆ—ï¼Œshape=(batch, seq_len, emb_size)
        """
        
        # ===== æ­¥éª¤1ï¼šä»æ¡ä»¶ç”Ÿæˆ AdaLN å‚æ•° =====
        # æ¡ä»¶å‘é‡ â†’ 6 ä¸ªå‚æ•°ï¼ˆ2ç»„ gamma, beta, alphaï¼‰
        gamma1_val = self.gamma1(cond)  # (batch, emb_size) - Attention çš„ scale
        beta1_val = self.beta1(cond)    # (batch, emb_size) - Attention çš„ shift
        alpha1_val = self.alpha1(cond)  # (batch, emb_size) - Attention çš„ gate
        gamma2_val = self.gamma2(cond)  # (batch, emb_size) - FFN çš„ scale
        beta2_val = self.beta2(cond)    # (batch, emb_size) - FFN çš„ shift
        alpha2_val = self.alpha2(cond)  # (batch, emb_size) - FFN çš„ gate
        
        # ===== æ­¥éª¤2ï¼šç¬¬ä¸€ä¸ª AdaLNï¼ˆSelf-Attention å‰ï¼‰=====
        # 2.1 Layer Normalization
        y = self.ln1(x)  # (batch, seq_len, emb_size)
        
        # 2.2 Scale & Shiftï¼ˆAdaLN çš„æ ¸å¿ƒï¼‰
        # y = gamma * y + beta
        # unsqueeze(1) æ˜¯ä¸ºäº†å¹¿æ’­åˆ°åºåˆ—ç»´åº¦ï¼š(batch, emb_size) â†’ (batch, 1, emb_size)
        y = y * (1 + gamma1_val.unsqueeze(1)) + beta1_val.unsqueeze(1)
        # (batch, seq_len, emb_size)
        # è§£é‡Šï¼šgamma æ§åˆ¶ç‰¹å¾ç¼©æ”¾ï¼Œbeta æ§åˆ¶ç‰¹å¾åç§»ï¼Œéƒ½ç”±æ¡ä»¶å†³å®š
        
        # ===== æ­¥éª¤3ï¼šMulti-Head Self-Attention =====
        # 3.1 ç”Ÿæˆ Q, K, V
        q = self.wq(y)  # (batch, seq_len, nhead * emb_size)
        k = self.wk(y)  # (batch, seq_len, nhead * emb_size)
        v = self.wv(y)  # (batch, seq_len, nhead * emb_size)
        
        # 3.2 é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        # (batch, seq_len, nhead*emb_size) â†’ (batch, nhead, seq_len, emb_size)
        q = q.view(q.size(0), q.size(1), self.nhead, self.emb_size).permute(0, 2, 1, 3)
        # Q: (batch, nhead, seq_len, emb_size)
        
        k = k.view(k.size(0), k.size(1), self.nhead, self.emb_size).permute(0, 2, 3, 1)
        # K: (batch, nhead, emb_size, seq_len) - æ³¨æ„æœ€åä¸¤ç»´è½¬ç½®äº†
        
        v = v.view(v.size(0), v.size(1), self.nhead, self.emb_size).permute(0, 2, 1, 3)
        # V: (batch, nhead, seq_len, emb_size)
        
        # 3.3 è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # Q @ K^Tï¼Œé™¤ä»¥ sqrt(d_k) è¿›è¡Œç¼©æ”¾
        attn = q @ k / math.sqrt(q.size(-1))  # (batch, nhead, seq_len, seq_len)
        # æ³¨æ„åŠ›çŸ©é˜µï¼šæ¯ä¸ª patch å¯¹å…¶ä»– patch çš„å…³æ³¨åº¦
        
        attn = torch.softmax(attn, dim=-1)  # (batch, nhead, seq_len, seq_len)
        # Softmax å½’ä¸€åŒ–ï¼Œæ¯è¡Œå’Œä¸º 1
        
        # 3.4 åº”ç”¨æ³¨æ„åŠ›åˆ° Value
        y = attn @ v  # (batch, nhead, seq_len, emb_size)
        # åŠ æƒæ±‚å’Œï¼šç”¨æ³¨æ„åŠ›åˆ†æ•°åŠ æƒæ‰€æœ‰ patch çš„ç‰¹å¾
        
        # 3.5 åˆå¹¶å¤šå¤´
        y = y.permute(0, 2, 1, 3)  # (batch, seq_len, nhead, emb_size)
        y = y.reshape(y.size(0), y.size(1), y.size(2) * y.size(3))
        # (batch, seq_len, nhead*emb_size)
        
        # 3.6 è¾“å‡ºæŠ•å½±
        y = self.lv(y)  # (batch, seq_len, emb_size)
        
        # ===== æ­¥éª¤4ï¼šAdaLN-Zeroï¼ˆé—¨æ§æ®‹å·®ï¼‰=====
        # alpha æ§åˆ¶ attention è¾“å‡ºçš„å¼ºåº¦
        y = y * alpha1_val.unsqueeze(1)  # (batch, seq_len, emb_size)
        # åˆå§‹è®­ç»ƒæ—¶ alpha æ¥è¿‘ 0ï¼Œæ¨¡å‹ä»æ’ç­‰æ˜ å°„å¼€å§‹å­¦ä¹ 
        
        # æ®‹å·®è¿æ¥
        y = x + y  # (batch, seq_len, emb_size)
        
        # ===== æ­¥éª¤5ï¼šç¬¬äºŒä¸ª AdaLNï¼ˆFeed-Forward å‰ï¼‰=====
        # 5.1 Layer Normalization
        z = self.ln2(y)  # (batch, seq_len, emb_size)
        
        # 5.2 Scale & Shift
        z = z * (1 + gamma2_val.unsqueeze(1)) + beta2_val.unsqueeze(1)
        # (batch, seq_len, emb_size)
        
        # ===== æ­¥éª¤6ï¼šFeed-Forward Network =====
        z = self.ff(z)  # (batch, seq_len, emb_size)
        # MLP: emb_size â†’ 4*emb_size â†’ emb_size
        
        # ===== æ­¥éª¤7ï¼šAdaLN-Zeroï¼ˆé—¨æ§æ®‹å·®ï¼‰=====
        z = z * alpha2_val.unsqueeze(1)  # (batch, seq_len, emb_size)
        
        # æœ€ç»ˆæ®‹å·®è¿æ¥
        return y + z  # (batch, seq_len, emb_size)
    
if __name__ == '__main__':
    # ===== æµ‹è¯•ä»£ç  =====
    print("ğŸ§ª æµ‹è¯• DiT Block")
    
    dit_block = DiTBlock(emb_size=16, nhead=4)
    
    # æ¨¡æ‹Ÿè¾“å…¥ï¼š5ä¸ªæ ·æœ¬ï¼Œ49ä¸ªpatchï¼Œæ¯ä¸ª16ç»´
    x = torch.rand((5, 49, 16))
    # æ¨¡æ‹Ÿæ¡ä»¶ï¼š5ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª16ç»´
    cond = torch.rand((5, 16))
    
    print(f"è¾“å…¥ x shape: {x.shape}")
    print(f"æ¡ä»¶ cond shape: {cond.shape}")
    
    outputs = dit_block(x, cond)
    
    print(f"è¾“å‡º shape: {outputs.shape}")
    print(f"âœ… æµ‹è¯•é€šè¿‡ï¼shape ä¿æŒä¸å˜")
    
    # éªŒè¯æ®‹å·®è¿æ¥
    print(f"\nğŸ” éªŒè¯æ®‹å·®è¿æ¥:")
    print(f"è¾“å…¥å’Œè¾“å‡ºçš„å‡å€¼å·®å¼‚: {(outputs - x).abs().mean():.6f}")
    print("ï¼ˆå·®å¼‚è¾ƒå°è¯´æ˜æ®‹å·®è¿æ¥èµ·ä½œç”¨ï¼‰")