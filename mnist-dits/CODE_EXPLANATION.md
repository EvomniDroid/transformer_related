# ğŸ“š DiT (Diffusion Transformer) ä»£ç è¯¦è§£

## ğŸ¯ æ•´ä½“æ¶æ„

DiT æ˜¯ä¸€ç§åŸºäº Transformer çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚ä¸ UNet ä¸åŒï¼ŒDiT å°†å›¾åƒåˆ‡åˆ†æˆ patches åç”¨ Transformer å¤„ç†ã€‚

```
è¾“å…¥: å¸¦å™ªå›¾åƒ x_t + æ—¶é—´æ­¥ t + æ ‡ç­¾ y
         â†“
    [Patchify] åˆ‡åˆ†æˆ patches
         â†“
    [Patch Embedding] æ˜ å°„åˆ°é«˜ç»´ç©ºé—´
         â†“
    [Position Encoding] æ·»åŠ ä½ç½®ä¿¡æ¯
         â†“
    [Time + Label Embedding] æ¡ä»¶ç¼–ç 
         â†“
    [DiT Blocks] Ã— N å±‚ Transformer
         â†“
    [Un-Patchify] é‡ç»„å›å›¾åƒ
         â†“
è¾“å‡º: é¢„æµ‹çš„å™ªéŸ³
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„

### **æ ¸å¿ƒæ¨¡å—**

| æ–‡ä»¶ | ä½œç”¨ | å…³é”®ç»„ä»¶ |
|------|------|---------|
| `dit.py` | DiT ä¸»æ¨¡å‹ | Patchify, Embedding, DiT Blocks, Un-patchify |
| `dit_block.py` | DiT Block | AdaLN, Self-Attention, FFN |
| `time_emb.py` | æ—¶é—´ç¼–ç  | æ­£å¼¦ä½ç½®ç¼–ç  |
| `diffusion.py` | æ‰©æ•£è¿‡ç¨‹ | å‰å‘åŠ å™ªï¼Œåå‘å»å™ª |
| `dataset.py` | æ•°æ®åŠ è½½ | MNIST æ•°æ®é›† |
| `config.py` | é…ç½®å‚æ•° | è¶…å‚æ•°è®¾ç½® |
| `train.py` | è®­ç»ƒè„šæœ¬ | è®­ç»ƒå¾ªç¯ |
| `inference.py` | æ¨ç†è„šæœ¬ | ç”Ÿæˆå›¾åƒ |

---

## ğŸ” æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

### **1. Patchifyï¼ˆå›¾åƒåˆ‡åˆ†ï¼‰**

```python
# 28Ã—28 å›¾åƒ â†’ 7Ã—7 ä¸ª 4Ã—4 patches
Conv2d(in_channels=1, out_channels=16, kernel_size=4, stride=4)
```

**è¿‡ç¨‹**ï¼š
```
åŸå›¾ (1, 28, 28)
    â†“ å·ç§¯ (kernel=4, stride=4)
Patches (16, 7, 7)  # 16 = 1 Ã— 4Ã—4
    â†“ permute + view
åºåˆ— (49, 16)  # 49 ä¸ª patchï¼Œæ¯ä¸ª 16 ç»´
```

---

### **2. AdaLN (Adaptive Layer Normalization)**

DiT çš„æ ¸å¿ƒåˆ›æ–°ï¼é€šè¿‡æ¡ä»¶ä¿¡æ¯åŠ¨æ€è°ƒæ•´å½’ä¸€åŒ–å‚æ•°ã€‚

**å…¬å¼**ï¼š
```
y = gamma(cond) * LayerNorm(x) + beta(cond)
output = x + alpha(cond) * Module(y)
```

**ä»£ç **ï¼š
```python
# ä»æ¡ä»¶ç”Ÿæˆå‚æ•°
gamma = self.gamma_linear(cond)  # scale
beta = self.beta_linear(cond)    # shift
alpha = self.alpha_linear(cond)  # gate

# åº”ç”¨ AdaLN
y = LayerNorm(x)
y = y * (1 + gamma) + beta

# æ¨¡å—å¤„ç†ï¼ˆAttention æˆ– FFNï¼‰
y = Module(y)

# é—¨æ§æ®‹å·®
output = x + alpha * y
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**
- `gamma`, `beta`: æ ¹æ®æ—¶é—´æ­¥å’Œæ ‡ç­¾è°ƒæ•´ç‰¹å¾åˆ†å¸ƒ
- `alpha`: æ§åˆ¶æ–°ä¿¡æ¯çš„æ³¨å…¥å¼ºåº¦ï¼Œåˆå§‹æ¥è¿‘ 0ï¼Œæ¨¡å‹ä»æ’ç­‰æ˜ å°„å¼€å§‹å­¦ä¹ 

---

### **3. Time Embeddingï¼ˆæ—¶é—´ç¼–ç ï¼‰**

ä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç å°†æ—¶é—´æ­¥ t âˆˆ [0, 999] ç¼–ç ä¸ºé«˜ç»´å‘é‡ã€‚

**å…¬å¼**ï¼š
```
PE(t, 2i)   = sin(t / 10000^(2i/d))
PE(t, 2i+1) = cos(t / 10000^(2i/d))
```

**ç‰¹ç‚¹**ï¼š
- ä¸åŒé¢‘ç‡æ•æ‰ä¸åŒç²’åº¦çš„æ—¶é—´ä¿¡æ¯
- ç›¸è¿‘æ—¶é—´æ­¥æœ‰ç›¸ä¼¼çš„ embedding
- å¯å­¦ä¹ çš„ MLP è¿›ä¸€æ­¥å¤„ç†

**ä»£ç æµç¨‹**ï¼š
```python
t = 999  # æ—¶é—´æ­¥
    â†“ é¢‘ç‡ç¼–ç 
[sin(999/1), sin(999/10), ..., cos(999/1), cos(999/10), ...]
    â†“ MLP
Time Embedding (64,)
```

---

### **4. Multi-Head Self-Attention**

æ ‡å‡† Transformer çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

**æ­¥éª¤**ï¼š
```python
# 1. ç”Ÿæˆ Q, K, V
Q = Linear(x)  # (batch, seq_len, nhead*emb_size)
K = Linear(x)
V = Linear(x)

# 2. æ‹†åˆ†æˆå¤šå¤´
Q = reshape(Q, (batch, nhead, seq_len, emb_size))

# 3. è®¡ç®—æ³¨æ„åŠ›
Attn = softmax(Q @ K^T / sqrt(d))  # (batch, nhead, seq_len, seq_len)

# 4. åº”ç”¨åˆ° Value
Output = Attn @ V  # (batch, nhead, seq_len, emb_size)

# 5. åˆå¹¶å¤šå¤´
Output = concat(Output)  # (batch, seq_len, nhead*emb_size)
Output = Linear(Output)  # (batch, seq_len, emb_size)
```

---

### **5. Un-Patchifyï¼ˆé‡ç»„å›¾åƒï¼‰**

å¤æ‚çš„ reshape æ“ä½œï¼Œå°† patch åºåˆ—è¿˜åŸä¸ºå›¾åƒã€‚

**ç»´åº¦å˜æ¢**ï¼š
```
(batch, 49, 16)  # 49 ä¸ª patchï¼Œæ¯ä¸ª 16 ç»´
    â†“ view
(batch, 7, 7, 1, 4, 4)  # 7Ã—7 ä¸ª patchï¼Œæ¯ä¸ª 1Ã—4Ã—4
    â†“ permute(0,3,1,2,4,5)
(batch, 1, 7, 7, 4, 4)  # channel åœ¨å‰
    â†“ permute(0,1,2,4,3,5)
(batch, 1, 7, 4, 7, 4)  # è°ƒæ•´ patch æ’åˆ—
    â†“ reshape
(batch, 1, 28, 28)  # å®Œæ•´å›¾åƒï¼
```

**å…³é”®æŠ€å·§**ï¼š
- å…ˆ view æ‹†åˆ† patch çš„å†…éƒ¨ç»“æ„
- ç”¨ permute è°ƒæ•´ç»´åº¦é¡ºåº
- æœ€å reshape åˆå¹¶ç›¸é‚»ç»´åº¦

---

## ğŸ”¢ Shape å˜åŒ–å…¨æµç¨‹

### **ä»¥ MNIST ä¸ºä¾‹ï¼ˆ28Ã—28 ç°åº¦å›¾ï¼‰**

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å…¥é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
x:     (batch, 1, 28, 28)   â† åŸå§‹å›¾åƒ
t:     (batch,)              â† æ—¶é—´æ­¥ [0-999]
y:     (batch,)              â† æ ‡ç­¾ [0-9]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Patchify é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Conv2d:        (batch, 16, 7, 7)
permute:       (batch, 7, 7, 16)
view:          (batch, 49, 16)
patch_emb:     (batch, 49, 64)
+ pos_emb:     (batch, 49, 64)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ¡ä»¶ç¼–ç é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
time_emb:      (batch, 64)
label_emb:     (batch, 64)
cond = t + y:  (batch, 64)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DiT Block å¤„ç†ï¼ˆÃ—3 å±‚ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å…¥:          (batch, 49, 64)
  â†“ AdaLN + Attention
ä¸­é—´:          (batch, 49, 64)
  â†“ AdaLN + FFN
è¾“å‡º:          (batch, 49, 64)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Un-Patchify é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
LayerNorm:     (batch, 49, 64)
Linear:        (batch, 49, 16)
view:          (batch, 7, 7, 1, 4, 4)
permute:       (batch, 1, 7, 7, 4, 4)
permute:       (batch, 1, 7, 4, 7, 4)
reshape:       (batch, 1, 28, 28)  âœ…

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å‡ºé˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
é¢„æµ‹å™ªéŸ³:      (batch, 1, 28, 28)
```

---

## ğŸ’¡ DiT vs ViT å¯¹æ¯”

| ç‰¹æ€§ | ViT | DiT |
|------|-----|-----|
| **ä»»åŠ¡** | å›¾åƒåˆ†ç±» | å›¾åƒç”Ÿæˆ |
| **è¾“å…¥** | æ¸…æ™°å›¾åƒ | å¸¦å™ªå›¾åƒ + æ—¶é—´æ­¥ + æ ‡ç­¾ |
| **CLS Token** | âœ… æœ‰ï¼ˆç”¨äºåˆ†ç±»ï¼‰ | âŒ æ— ï¼ˆæ‰€æœ‰ patch éƒ½ç”¨ï¼‰ |
| **æ¡ä»¶ä¿¡æ¯** | æ—  | æ—¶é—´æ­¥ t + æ ‡ç­¾ y |
| **å½’ä¸€åŒ–** | LayerNorm | AdaLNï¼ˆè‡ªé€‚åº”ï¼‰ |
| **è¾“å‡º** | åˆ†ç±» logits | å®Œæ•´å›¾åƒ |
| **Un-patchify** | ä¸éœ€è¦ | âœ… éœ€è¦é‡ç»„å›¾åƒ |

---

## ğŸ§ª å…³é”®ä»£ç ç‰‡æ®µ

### **AdaLN çš„æ ¸å¿ƒå®ç°**

```python
# ç”Ÿæˆæ¡ä»¶å‚æ•°
gamma = self.gamma(cond)  # (batch, emb_size)
beta = self.beta(cond)    # (batch, emb_size)
alpha = self.alpha(cond)  # (batch, emb_size)

# AdaLN: scale & shift
y = LayerNorm(x)  # (batch, seq_len, emb_size)
y = y * (1 + gamma.unsqueeze(1)) + beta.unsqueeze(1)

# æ¨¡å—å¤„ç†ï¼ˆSelf-Attention æˆ– FFNï¼‰
y = Module(y)

# AdaLN-Zero: é—¨æ§æ®‹å·®
output = x + alpha.unsqueeze(1) * y
```

### **Un-patchify çš„å®Œæ•´è¿‡ç¨‹**

```python
# (batch, 49, 16) â†’ (batch, 1, 28, 28)

x = x.view(batch, 7, 7, 1, 4, 4)     # æ‹†åˆ† patch ç»“æ„
x = x.permute(0, 3, 1, 2, 4, 5)      # channel åœ¨å‰
x = x.permute(0, 1, 2, 4, 3, 5)      # è°ƒæ•´ patch æ’åˆ—
x = x.reshape(batch, 1, 28, 28)      # åˆå¹¶ç»´åº¦
```

---

## ğŸ“Š æ¨¡å‹å‚æ•°é‡ä¼°ç®—

**ä»¥é»˜è®¤é…ç½®ä¸ºä¾‹**ï¼š
- img_size=28, patch_size=4, emb_size=64, dit_num=3, head=4

```
Patchify:
  - Conv2d: 1Ã—16Ã—4Ã—4 = 256
  - Linear: 16Ã—64 = 1,024
  - pos_emb: 49Ã—64 = 3,136

Conditioning:
  - time_emb MLP: ~12K
  - label_emb: 10Ã—64 = 640

DiT Blocks (Ã—3):
  - AdaLN å‚æ•°: 6Ã—(64Ã—64) = 24,576 per block
  - Attention: 3Ã—(64Ã—256) = 49,152 per block
  - FFN: 64Ã—256 + 256Ã—64 = 32,768 per block
  - æ¯ä¸ª Block â‰ˆ 106K
  - 3 ä¸ª Block â‰ˆ 318K

Output:
  - LayerNorm: 128
  - Linear: 64Ã—16 = 1,024

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»è®¡: ~350K å‚æ•°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ å­¦ä¹ å»ºè®®

### **ç†è§£é¡ºåº**

1. **ç¬¬ä¸€æ­¥**: ç†è§£ `time_emb.py`ï¼ˆæœ€ç®€å•ï¼‰
2. **ç¬¬äºŒæ­¥**: ç†è§£ `dit_block.py`ï¼ˆæ ¸å¿ƒæœºåˆ¶ï¼‰
3. **ç¬¬ä¸‰æ­¥**: ç†è§£ `dit.py` çš„ patchify å’Œ un-patchify
4. **ç¬¬å››æ­¥**: ç†è§£å®Œæ•´çš„å‰å‘ä¼ æ’­æµç¨‹

### **è°ƒè¯•æŠ€å·§**

åœ¨å…³é”®ä½ç½®æ·»åŠ  shape æ‰“å°ï¼š
```python
print(f"After patchify: {x.shape}")
print(f"After DiT blocks: {x.shape}")
print(f"After un-patchify: {x.shape}")
```

### **åŠ¨æ‰‹å®éªŒ**

1. ä¿®æ”¹ `patch_size` ä¸º 7ï¼Œè§‚å¯Ÿ shape å˜åŒ–
2. å¢åŠ  `dit_num` åˆ° 6ï¼Œè§‚å¯Ÿå‚æ•°é‡å˜åŒ–
3. å¯è§†åŒ–ä¸åŒæ—¶é—´æ­¥çš„ time embedding

---

## ğŸ”— å‚è€ƒèµ„æ–™

- **DiT è®ºæ–‡**: [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)
- **ViT è®ºæ–‡**: [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)
- **DDPM**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)

---

**ç¥ä½ å­¦ä¹ é¡ºåˆ©ï¼æœ‰é—®é¢˜éšæ—¶æŸ¥é˜…æ³¨é‡Šæˆ–æ–‡æ¡£** ğŸ‰
