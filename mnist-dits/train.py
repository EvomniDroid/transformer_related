"""
DiT æ¨¡å‹è®­ç»ƒè„šæœ¬

è®­ç»ƒæµç¨‹ï¼š
1. ä»æ•°æ®é›†åŠ è½½æ¸…æ™°å›¾åƒ x_0
2. éšæœºé‡‡æ ·æ—¶é—´æ­¥ t
3. ä½¿ç”¨å‰å‘æ‰©æ•£æ·»åŠ å™ªå£°ï¼šx_0 â†’ x_t
4. ç”¨ DiT æ¨¡å‹é¢„æµ‹å™ªå£°
5. è®¡ç®—é¢„æµ‹å™ªå£°ä¸çœŸå®å™ªå£°çš„æŸå¤±
6. åå‘ä¼ æ’­æ›´æ–°æ¨¡å‹å‚æ•°
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # è§£å†³ OpenMP åº“å†²çª

from config import *
from torch.utils.data import DataLoader
from dataset import MNIST
from diffusion import forward_add_noise
import torch 
from torch import nn 
from dit import DiT

# ===== è®¾å¤‡é…ç½® =====
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# ===== æ•°æ®é›† =====
dataset = MNIST()
print(f"ğŸ“¦ æ•°æ®é›†å¤§å°: {len(dataset)}")

# ===== æ¨¡å‹åˆå§‹åŒ– =====
model = DiT(
    img_size=28,      # MNIST å›¾åƒå¤§å°
    patch_size=4,     # Patch å¤§å°
    channel=1,        # ç°åº¦å›¾
    emb_size=64,      # Embedding ç»´åº¦
    label_num=10,     # 10 ä¸ªæ•°å­—ç±»åˆ«
    dit_num=3,        # 3 å±‚ DiT Block
    head=4            # 4 ä¸ªæ³¨æ„åŠ›å¤´
).to(DEVICE)

print(f"ğŸ¤– æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# ===== åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰=====
try:
    model.load_state_dict(torch.load('model.pth'))
    print("âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æˆåŠŸ")
except:
    print("âš ï¸  æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

# ===== ä¼˜åŒ–å™¨ =====
# Adam ä¼˜åŒ–å™¨ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•
# - model.parameters(): æ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
# - lr=1e-3: å­¦ä¹ ç‡ 0.001
optimzer = torch.optim.Adam(model.parameters(), lr=1e-3)
print(f"ğŸ”§ ä¼˜åŒ–å™¨: Adam, å­¦ä¹ ç‡: 1e-3")

# ===== æŸå¤±å‡½æ•° =====
# L1Loss (Mean Absolute Error): è®¡ç®—é¢„æµ‹å™ªå£°å’ŒçœŸå®å™ªå£°çš„ç»å¯¹å€¼è¯¯å·®
# Loss = mean(|pred_noise - true_noise|)
loss_fn = nn.L1Loss()
print(f"ğŸ“‰ æŸå¤±å‡½æ•°: L1Loss (MAE)")

# ===== è®­ç»ƒå¾ªç¯ =====
if __name__ == '__main__':
    # è®­ç»ƒå‚æ•°
    EPOCH = 500        # è®­ç»ƒè½®æ•°
    BATCH_SIZE = 300   # æ‰¹æ¬¡å¤§å°
    
    # Windows ä¸‹ä½¿ç”¨ num_workers=0 é¿å…å¤šè¿›ç¨‹é—®é¢˜
    dataloader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,          # æ¯ä¸ª epoch æ‰“ä¹±æ•°æ®
        num_workers=0          # Windows å…¼å®¹æ€§
    )
    
    # ===== æŸ¥çœ‹å•å¼ å›¾ç‰‡çš„è¯¦ç»†ä¿¡æ¯ =====
    for inputs, labels in dataloader:
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®æ‰¹æ¬¡ä¿¡æ¯:")
        print(f"  - Batch å¤§å°: {inputs.shape[0]} å¼ å›¾ç‰‡")
        print(f"  - å›¾ç‰‡å½¢çŠ¶: {inputs.shape}  (batch, channel, height, width)")
        print(f"  - æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        print("="*60)
        
        # åªçœ‹ç¬¬ä¸€å¼ å›¾ç‰‡
        first_img = inputs[0]      # å½¢çŠ¶: (1, 28, 28)
        first_label = labels[0]    # å½¢çŠ¶: æ ‡é‡
        
        print(f"\nğŸ–¼ï¸  ç¬¬ä¸€å¼ å›¾ç‰‡çš„æ ‡ç­¾: {first_label.item()} (æ•°å­— {first_label.item()})")
        print(f"   å›¾ç‰‡å½¢çŠ¶: {first_img.shape}")
        print(f"   åƒç´ å€¼èŒƒå›´: [{first_img.min():.4f}, {first_img.max():.4f}]")
        
        # æ˜¾ç¤ºå›¾ç‰‡çš„åƒç´ çŸ©é˜µ (28x28)
        print(f"\n   åƒç´ çŸ©é˜µ (28x28):")
        print(f"   {'â”€'*56}")
        # å»æ‰ channel ç»´åº¦,åªçœ‹äºŒç»´çŸ©é˜µ
        img_2d = first_img.squeeze(0)  # (28, 28)
        
        # æ‰“å°å‰ 10 è¡Œ,æ¯è¡Œå‰ 10 ä¸ªåƒç´ 
        for i in range(10):
            row = img_2d[i, :10]
            print(f"   Row {i:2d}: [{', '.join([f'{x:.2f}' for x in row])} ...]")
        print(f"   {'â”€'*56}")
        print(f"   (ä»…æ˜¾ç¤ºå‰ 10x10 åƒç´ ,å®Œæ•´å›¾ç‰‡æ˜¯ 28x28)")
        
        # æŸ¥çœ‹å“ªäº›ä½ç½®æœ‰éé›¶åƒç´  (æ•°å­—ç¬”ç”»çš„ä½ç½®)
        nonzero_count = (img_2d > 0).sum().item()
        print(f"\n   âœï¸  éé›¶åƒç´ æ•°é‡: {nonzero_count} / 784 ({nonzero_count/784*100:.1f}%)")
        print(f"   ğŸ’¡ è¿™äº›éé›¶åƒç´ å°±æ˜¯æ•°å­— '{first_label.item()}' çš„ç¬”ç”»éƒ¨åˆ†")
        
        break  # åªçœ‹ç¬¬ä¸€ä¸ª batch
    
    import pdb; pdb.set_trace()
    # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ Dropout, BatchNorm ç­‰ï¼‰
    model.train()
    
    iter_count = 0  # è¿­ä»£è®¡æ•°å™¨
    
    # ===== Epoch å¾ªç¯ =====
    for epoch in range(EPOCH):
        epoch_loss = 0.0  # è®°å½•å½“å‰ epoch çš„æ€»æŸå¤±
        batch_count = 0
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{EPOCH}")
  
        # ===== Batch å¾ªç¯ =====
        for imgs, labels in dataloader:
            # imgs: (batch_size, 1, 28, 28), å–å€¼èŒƒå›´ [0, 1]
            # labels: (batch_size,), ç±»åˆ«æ ‡ç­¾ 0-9
            
            # ===== æ­¥éª¤1: å›¾åƒæ ‡å‡†åŒ– =====
            # å°†åƒç´ å€¼ä» [0, 1] è½¬æ¢åˆ° [-1, 1]
            # è¿™æ ·å¯ä»¥å’Œé«˜æ–¯å™ªå£° N(0, 1) çš„èŒƒå›´åŒ¹é…
            x = imgs * 2 - 1  # (batch_size, 1, 28, 28), èŒƒå›´ [-1, 1]
            
            # ===== æ­¥éª¤2: éšæœºé‡‡æ ·æ—¶é—´æ­¥ =====
            # ä¸ºæ¯å¼ å›¾ç‰‡éšæœºé€‰æ‹©ä¸€ä¸ªæ—¶é—´æ­¥ t âˆˆ [0, T-1]
            # è¿™æ ·æ¨¡å‹å¯ä»¥å­¦ä¹ åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹é¢„æµ‹å™ªå£°
            t = torch.randint(0, T, (imgs.size(0),))  # (batch_size,)
            
            # ===== æ­¥éª¤3: å‡†å¤‡æ¡ä»¶æ ‡ç­¾ =====
            y = labels  # (batch_size,)
            
            # ===== æ­¥éª¤4: å‰å‘æ‰©æ•£ï¼ˆåŠ å™ªï¼‰=====
            # x_0 â†’ x_tï¼ŒåŒæ—¶è¿”å›æ·»åŠ çš„å™ªå£°
            x_noisy, noise = forward_add_noise(x, t)
            # x_noisy: (batch_size, 1, 28, 28) - åŠ å™ªåçš„å›¾åƒ
            # noise: (batch_size, 1, 28, 28) - æ·»åŠ çš„å™ªå£°ï¼ˆground truthï¼‰
            
            # ===== æ­¥éª¤5: æ¨¡å‹é¢„æµ‹å™ªå£° =====
            # è¾“å…¥ï¼šå¸¦å™ªå›¾åƒ x_t, æ—¶é—´æ­¥ t, æ ‡ç­¾ y
            # è¾“å‡ºï¼šé¢„æµ‹çš„å™ªå£°
            pred_noise = model(
                x_noisy.to(DEVICE), 
                t.to(DEVICE), 
                y.to(DEVICE)
            )  # (batch_size, 1, 28, 28)
            
            # ===== æ­¥éª¤6: è®¡ç®—æŸå¤± =====
            # L1 æŸå¤±ï¼š|é¢„æµ‹å™ªå£° - çœŸå®å™ªå£°|
            loss = loss_fn(pred_noise, noise.to(DEVICE))
            
            # ===== æ­¥éª¤7: åå‘ä¼ æ’­ =====
            optimzer.zero_grad()  # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
            loss.backward()        # è®¡ç®—æ¢¯åº¦
            optimzer.step()        # æ›´æ–°å‚æ•°
            
            # ===== è®°å½•æŸå¤± =====
            epoch_loss += loss.item()
            batch_count += 1
            
            # ===== å®šæœŸä¿å­˜æ¨¡å‹å’Œæ‰“å°æ—¥å¿— =====
            if iter_count % 100 == 0:
                print(f"  Iter {iter_count:5d} | Loss: {loss.item():.6f}")
            
            if iter_count % 1000 == 0:
                # ä¿å­˜æ¨¡å‹
                torch.save(model.state_dict(), 'model.pth')
                print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ (iter={iter_count})")
            
            iter_count += 1
        
        # ===== Epoch ç»“æŸç»Ÿè®¡ =====
        avg_loss = epoch_loss / batch_count
        print(f"  âœ… Epoch {epoch+1} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.6f}")
        
        # æ¯ä¸ª epoch ç»“æŸä¿å­˜ä¸€æ¬¡
        torch.save(model.state_dict(), 'model.pth')
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*60}")
    print(f"æ€»è¿­ä»£æ¬¡æ•°: {iter_count}")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: model.pth")