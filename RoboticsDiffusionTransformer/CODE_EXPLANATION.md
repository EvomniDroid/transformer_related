# ğŸ“š RDT (Robotics Diffusion Transformer) ä»£ç è¯¦è§£

## ğŸ¯ æ•´ä½“æ¶æ„

RDT æ˜¯ä¸€ç§**ä¸“ä¸ºæœºå™¨äººæ§åˆ¶ä»»åŠ¡è®¾è®¡**çš„å¤šæ¨¡æ€æ‰©æ•£ Transformer æ¨¡å‹ã€‚å®ƒç»“åˆäº†è¯­è¨€æŒ‡ä»¤ã€è§†è§‰è§‚æµ‹å’ŒçŠ¶æ€ä¿¡æ¯,ç”Ÿæˆæœºå™¨äººåŠ¨ä½œåºåˆ—ã€‚

```
è¾“å…¥: 
  - å¸¦å™ªåŠ¨ä½œåºåˆ— x_t (horizon ä¸ªåŠ¨ä½œ)
  - æ—¶é—´æ­¥ t (æ‰©æ•£æ—¶é—´)
  - æ§åˆ¶é¢‘ç‡ freq
  - å½“å‰çŠ¶æ€ state
  - è¯­è¨€æŒ‡ä»¤ lang_c (æ–‡æœ¬ embedding)
  - å›¾åƒè§‚æµ‹ img_c (è§†è§‰ embedding)
         â†“
    [Time + Freq Embedding] æ—¶é—´å’Œé¢‘ç‡ç¼–ç 
         â†“
    [Position Embedding] ä½ç½®ç¼–ç 
         â†“
    [RDT Blocks] Ã— 28 å±‚ äº¤æ›¿ä½¿ç”¨è¯­è¨€å’Œè§†è§‰æ¡ä»¶
         â”‚
         â”œâ”€ è¯­è¨€æ¡ä»¶å±‚: Self-Attn + Cross-Attn(è¯­è¨€) + FFN
         â””â”€ è§†è§‰æ¡ä»¶å±‚: Self-Attn + Cross-Attn(è§†è§‰) + FFN
         â†“
    [Final Layer] è¾“å‡ºå±‚
         â†“
è¾“å‡º: é¢„æµ‹çš„å™ªå£°æˆ–å»å™ªåçš„åŠ¨ä½œåºåˆ—
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„

### **æ ¸å¿ƒæ¨¡å—**

| æ–‡ä»¶ | ä½œç”¨ | å…³é”®ç»„ä»¶ |
|------|------|---------|
| `models/rdt/model.py` | RDT ä¸»æ¨¡å‹ | ä½ç½®ç¼–ç , RDT Blocks å †å , æœ€ç»ˆè¾“å‡ºå±‚ |
| `models/rdt/blocks.py` | RDT Block å’Œç»„ä»¶ | TimestepEmbedder, CrossAttention, RDTBlock, FinalLayer |
| `models/multimodal_encoder/` | å¤šæ¨¡æ€ç¼–ç å™¨ | è¯­è¨€ç¼–ç å™¨(T5/CLIP), è§†è§‰ç¼–ç å™¨(ResNet/ViT) |
| `models/rdt_runner.py` | æ‰©æ•£é‡‡æ ·å™¨ | DDPM, DDIM é‡‡æ ·ç®—æ³• |
| `models/ema_model.py` | EMA æ¨¡å‹ | æŒ‡æ•°ç§»åŠ¨å¹³å‡,ç¨³å®šè®­ç»ƒ |
| `train/train.py` | è®­ç»ƒè„šæœ¬ | å¤šæ¨¡æ€æ•°æ®åŠ è½½, è®­ç»ƒå¾ªç¯ |

---

## ğŸ” æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

### **1. å¤šæ¨¡æ€æ¡ä»¶è¾“å…¥**

RDT å¤„ç†**ä¸‰ç§**æ¡ä»¶ä¿¡æ¯,è¿™æ˜¯ä¸ DiT æœ€å¤§çš„åŒºåˆ«!

```python
è¾“å…¥ç»´åº¦è¯´æ˜:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¸»åºåˆ— x: (B, T+1, D)
  - T = horizon (åŠ¨ä½œåºåˆ—é•¿åº¦, å¦‚ 32 æ­¥)
  - T+1 åŒ…å«: [timestep, freq, state, action_1, ..., action_T]
  - D = hidden_size (å¦‚ 1152)

è¯­è¨€æ¡ä»¶ lang_c: (B, L_lang, D)
  - L_lang â‰¤ 1024 (è¯­è¨€ token æ•°é‡,å¯å˜é•¿åº¦)
  - ä¾‹å¦‚: "æ‹¿èµ·æ¡Œä¸Šçš„çº¢è‰²æ¯å­"

å›¾åƒæ¡ä»¶ img_c: (B, L_img, D)
  - L_img = 4096 (å›¾åƒ token æ•°é‡,å›ºå®šé•¿åº¦)
  - ä¾‹å¦‚: ä» ResNet æå–çš„è§†è§‰ç‰¹å¾
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€?**
- **è¯­è¨€**: ç†è§£ä»»åŠ¡ç›®æ ‡ ("æ‹¿èµ·æ¯å­")
- **è§†è§‰**: æ„ŸçŸ¥ç¯å¢ƒçŠ¶æ€ (æ¯å­åœ¨å“ªé‡Œ?)
- **çŠ¶æ€**: æœºå™¨äººå½“å‰å§¿æ€ (å…³èŠ‚è§’åº¦, ä½ç½®ç­‰)

---

### **2. äº¤æ›¿çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶**

RDT çš„åˆ›æ–°ä¹‹å¤„:**å¥‡æ•°å±‚ç”¨è¯­è¨€æ¡ä»¶,å¶æ•°å±‚ç”¨è§†è§‰æ¡ä»¶**

```python
# 28 å±‚ RDT Blocks äº¤æ›¿ä½¿ç”¨æ¡ä»¶
for i, block in enumerate(self.blocks):
    if i % 2 == 0:
        c = lang_c      # å¶æ•°å±‚: è¯­è¨€æ¡ä»¶
        mask = lang_mask
    else:
        c = img_c       # å¥‡æ•°å±‚: è§†è§‰æ¡ä»¶
        mask = img_mask
    x = block(x, c, mask)
```

**ä¸ºä»€ä¹ˆè¦äº¤æ›¿?**
1. **ä¿¡æ¯èåˆ**: è¯­è¨€å’Œè§†è§‰é€å±‚äº¤æ›¿èå…¥,å……åˆ†ç»“åˆ
2. **è®¡ç®—æ•ˆç‡**: ä¸éœ€è¦æ¯å±‚åŒæ—¶å¤„ç†ä¸¤ç§æ¡ä»¶
3. **æ¢¯åº¦æµåŠ¨**: æ¯ç§æ¡ä»¶éƒ½æœ‰è¶³å¤Ÿçš„æ›´æ–°æœºä¼š

**å…·ä½“æµç¨‹:**
```
è¾“å…¥ x: [timestep, freq, state, action_1, ..., action_32]

Layer 0:  Self-Attn â†’ Cross-Attn(è¯­è¨€) â†’ FFN
          â†“ ç†è§£ä»»åŠ¡æŒ‡ä»¤
Layer 1:  Self-Attn â†’ Cross-Attn(è§†è§‰) â†’ FFN
          â†“ æ„ŸçŸ¥è§†è§‰ç¯å¢ƒ
Layer 2:  Self-Attn â†’ Cross-Attn(è¯­è¨€) â†’ FFN
          â†“ å†æ¬¡å…³æ³¨è¯­è¨€ç»†èŠ‚
Layer 3:  Self-Attn â†’ Cross-Attn(è§†è§‰) â†’ FFN
          â†“ å†æ¬¡å…³æ³¨è§†è§‰ç»†èŠ‚
...
Layer 27: Self-Attn â†’ Cross-Attn(è§†è§‰) â†’ FFN

è¾“å‡º: å»å™ªåçš„åŠ¨ä½œåºåˆ—
```

---

### **3. RDT Block ç»“æ„**

æ¯ä¸ª RDT Block åŒ…å«**ä¸‰ä¸ª**å­æ¨¡å—:

```python
class RDTBlock:
    def forward(self, x, c, mask):
        # 1ï¸âƒ£ Self-Attention: åŠ¨ä½œåºåˆ—å†…éƒ¨çš„å…³ç³»
        x = x + Attention(RmsNorm(x))
        
        # 2ï¸âƒ£ Cross-Attention: åŠ¨ä½œåºåˆ— attend to æ¡ä»¶ (è¯­è¨€æˆ–è§†è§‰)
        x = x + CrossAttention(RmsNorm(x), c, mask)
        
        # 3ï¸âƒ£ FFN: éçº¿æ€§å˜æ¢
        x = x + FFN(RmsNorm(x))
        
        return x
```

**ä¸ DiT Block çš„å¯¹æ¯”:**

| ç‰¹æ€§ | DiT Block | RDT Block |
|------|-----------|-----------|
| **å½’ä¸€åŒ–** | LayerNorm + AdaLN | RmsNorm (æ›´ç¨³å®š) |
| **æ¡ä»¶æœºåˆ¶** | AdaLN (è°ƒåˆ¶å½’ä¸€åŒ–) | Cross-Attention (æ˜¾å¼æ³¨æ„åŠ›) |
| **æ¡ä»¶ç±»å‹** | å•ä¸€æ¡ä»¶ (æ—¶é—´+æ ‡ç­¾) | åŒæ¡ä»¶äº¤æ›¿ (è¯­è¨€/è§†è§‰) |
| **æ®‹å·®è¿æ¥** | AdaLN-Zero (é—¨æ§) | ç›´æ¥ç›¸åŠ  |
| **FFN æ¿€æ´»** | GELU | GELU(tanh è¿‘ä¼¼) |

---

### **4. Cross-Attention è¯¦è§£**

Cross-Attention æ˜¯ RDT çš„æ ¸å¿ƒæœºåˆ¶,ç”¨äºæ³¨å…¥æ¡ä»¶ä¿¡æ¯ã€‚

**è®¡ç®—æµç¨‹:**
```python
# è¾“å…¥:
#   x: (B, N, D) - åŠ¨ä½œåºåˆ— (Query)
#   c: (B, L, D) - æ¡ä»¶åºåˆ— (Key, Value)
#   mask: (B, L) - æœ‰æ•ˆ token æ©ç 

# 1. ç”Ÿæˆ Q, K, V
Q = Linear_q(x)    # (B, N, D) - ä»åŠ¨ä½œåºåˆ—ç”Ÿæˆ
K = Linear_k(c)    # (B, L, D) - ä»æ¡ä»¶ç”Ÿæˆ
V = Linear_v(c)    # (B, L, D) - ä»æ¡ä»¶ç”Ÿæˆ

# 2. æ‹†åˆ†æˆå¤šå¤´
Q = Q.reshape(B, num_heads, N, head_dim)
K = K.reshape(B, num_heads, L, head_dim)
V = V.reshape(B, num_heads, L, head_dim)

# 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
Attn = softmax(Q @ K^T / sqrt(head_dim))  # (B, num_heads, N, L)
# â†‘ æ¯ä¸ªåŠ¨ä½œ token å¯¹æ‰€æœ‰æ¡ä»¶ token çš„æ³¨æ„åŠ›æƒé‡

# 4. åº”ç”¨æ©ç  (å±è”½æ— æ•ˆçš„æ¡ä»¶ token)
if mask is not None:
    Attn = Attn.masked_fill(~mask, -inf)

# 5. åŠ æƒæ±‚å’Œ
Output = Attn @ V  # (B, num_heads, N, head_dim)

# 6. åˆå¹¶å¤šå¤´
Output = Output.reshape(B, N, D)
```

**æ©ç çš„ä½œç”¨:**
```python
# è¯­è¨€æ¡ä»¶å¯èƒ½æ˜¯å˜é•¿çš„
lang_c = [
    "pick up the red cup",        # 5 ä¸ª token
    "grasp the object",           # 3 ä¸ª token (padding åˆ° 1024)
]
lang_mask = [
    [1,1,1,1,1, 0,0,0,...,0],    # True è¡¨ç¤ºæœ‰æ•ˆ, False è¡¨ç¤º padding
    [1,1,1, 0,0,0,...,0],
]

# Cross-Attention æ—¶åªå…³æ³¨æœ‰æ•ˆçš„ token,å¿½ç•¥ padding
```

---

### **5. Position Embedding (ä½ç½®ç¼–ç )**

RDT ä½¿ç”¨**ä¸‰å¥—**ç‹¬ç«‹çš„ä½ç½®ç¼–ç :

```python
# 1ï¸âƒ£ ä¸»åºåˆ—ä½ç½®ç¼–ç  (åŠ¨ä½œåºåˆ—)
self.x_pos_embed: (1, horizon+3, hidden_size)
# horizon+3 = [timestep, freq, state] + horizon ä¸ªåŠ¨ä½œ

# 2ï¸âƒ£ è¯­è¨€æ¡ä»¶ä½ç½®ç¼–ç 
self.lang_cond_pos_embed: (1, 1024, hidden_size)
# æ”¯æŒæœ€å¤š 1024 ä¸ªè¯­è¨€ token

# 3ï¸âƒ£ è§†è§‰æ¡ä»¶ä½ç½®ç¼–ç 
self.img_cond_pos_embed: (1, 4096, hidden_size)
# 4096 ä¸ªè§†è§‰ token (ä¾‹å¦‚ 64x64 çš„ feature map)
```

**å¤šæ¨¡æ€ä½ç½®ç¼–ç çš„è®¾è®¡:**
```python
# ä¸»åºåˆ—ä½¿ç”¨åˆ†æ®µç¼–ç 
x_pos_embed = get_multimodal_cond_pos_embed(
    embed_dim=1152,
    mm_cond_lens=OrderedDict([
        ('timestep', 1),   # æ—¶é—´æ­¥å  1 ä¸ªä½ç½®
        ('ctrl_freq', 1),  # æ§åˆ¶é¢‘ç‡å  1 ä¸ªä½ç½®
        ('state', 1),      # çŠ¶æ€å  1 ä¸ªä½ç½®
        ('action', 32),    # åŠ¨ä½œå  32 ä¸ªä½ç½®
    ])
)

# ä¸åŒæ¨¡æ€æœ‰ä¸åŒçš„é¢‘ç‡ç¼–ç ,å¸®åŠ©æ¨¡å‹åŒºåˆ†
```

---

### **6. Timestep Embedding (æ—¶é—´ç¼–ç )**

ä¸ DiT ç±»ä¼¼,ä½¿ç”¨æ­£å¼¦ç¼–ç :

```python
class TimestepEmbedder:
    def timestep_embedding(self, t, dim=256):
        # t: (B,) æ—¶é—´æ­¥ [0, 999]
        half = dim // 2
        freqs = exp(-log(10000) * arange(0, half) / half)
        # freqs: [1, 0.87, 0.76, ..., 0.001]
        
        args = t[:, None] * freqs[None, :]
        embedding = concat([cos(args), sin(args)])  # (B, 256)
        return embedding
    
    def forward(self, t):
        t_freq = self.timestep_embedding(t, 256)  # (B, 256)
        t_emb = MLP(t_freq)                       # (B, 1152)
        return t_emb
```

**RDT æœ‰ä¸¤ä¸ªæ—¶é—´ç¼–ç å™¨:**
1. `t_embedder`: æ‰©æ•£æ—¶é—´æ­¥ (0-999)
2. `freq_embedder`: æ§åˆ¶é¢‘ç‡ (ä¾‹å¦‚ 10Hz, 20Hz)

---

### **7. Final Layer (è¾“å‡ºå±‚)**

```python
class FinalLayer:
    def forward(self, x):
        # x: (B, horizon+3, hidden_size)
        x = RmsNorm(x)                    # å½’ä¸€åŒ–
        x = FFN(x)                        # æ˜ å°„åˆ°è¾“å‡ºç»´åº¦
        # x: (B, horizon+3, output_dim)
        return x

# åœ¨ä¸»æ¨¡å‹ä¸­:
x = self.final_layer(x)        # (B, horizon+3, output_dim)
x = x[:, -self.horizon:]       # åªä¿ç•™åŠ¨ä½œéƒ¨åˆ†
                               # (B, horizon, output_dim)
```

**è¾“å‡ºç»´åº¦:**
- `output_dim = 128`: æœºå™¨äººåŠ¨ä½œçš„ç»´åº¦
  - ä¾‹å¦‚: 7 ä¸ªå…³èŠ‚è§’åº¦ + 1 ä¸ªå¤¹çˆª = 8 ç»´
  - å¦‚æœé¢„æµ‹ 16 æ­¥,åˆ™ 8 Ã— 16 = 128

---

## ğŸ”¢ Shape å˜åŒ–å…¨æµç¨‹

### **ä»¥ RDT-1B ä¸ºä¾‹**

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å…¥é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
x:           (B, 33, 1152)      â† state + 32 ä¸ªåŠ¨ä½œ
t:           (B,)               â† æ‰©æ•£æ—¶é—´æ­¥ [0, 999]
freq:        (B,)               â† æ§åˆ¶é¢‘ç‡ [1, 50]
lang_c:      (B, L_lang, 1152)  â† è¯­è¨€æ¡ä»¶ (å˜é•¿)
img_c:       (B, 4096, 1152)    â† è§†è§‰æ¡ä»¶ (å›ºå®šé•¿åº¦)
lang_mask:   (B, L_lang)        â† è¯­è¨€æ©ç 
img_mask:    (B, 4096)          â† è§†è§‰æ©ç 

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Embedding é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
t_emb:       (B, 1152)
             â†“ unsqueeze
             (B, 1, 1152)

freq_emb:    (B, 1152)
             â†“ unsqueeze
             (B, 1, 1152)

x:           (B, 33, 1152)
             â†“ concat([t_emb, freq_emb, x])
             (B, 35, 1152)  â† [t, freq, state, action_1, ..., action_32]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Position Embedding é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
x:           (B, 35, 1152)
+ x_pos_emb: (1, 35, 1152)     â† å¹¿æ’­åŠ æ³•
â†’            (B, 35, 1152)

lang_c:      (B, L_lang, 1152)
+ lang_pos:  (1, L_lang, 1152) â† åªå–å‰ L_lang ä¸ªä½ç½®
â†’            (B, L_lang, 1152)

img_c:       (B, 4096, 1152)
+ img_pos:   (1, 4096, 1152)
â†’            (B, 4096, 1152)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RDT Blocks å¤„ç† (Ã— 28 å±‚)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Layer 0:  (å¶æ•°å±‚ - è¯­è¨€æ¡ä»¶)
  è¾“å…¥ x:        (B, 35, 1152)
  æ¡ä»¶ lang_c:   (B, L_lang, 1152)
  
  â†“ Self-Attention
  Q,K,V from x:  (B, 16, 35, 72)      â† 16 heads, 72 = 1152/16
  Attn:          (B, 16, 35, 35)
  Output:        (B, 35, 1152)
  
  â†“ Cross-Attention (è¯­è¨€)
  Q from x:      (B, 16, 35, 72)
  K,V from lang: (B, 16, L_lang, 72)
  Attn:          (B, 16, 35, L_lang)  â† 35 ä¸ª query, L_lang ä¸ª key
  Output:        (B, 35, 1152)
  
  â†“ FFN
  Input:         (B, 35, 1152)
  Hidden:        (B, 35, 1152)        â† FFN hidden = input
  Output:        (B, 35, 1152)

Layer 1:  (å¥‡æ•°å±‚ - è§†è§‰æ¡ä»¶)
  è¾“å…¥ x:        (B, 35, 1152)
  æ¡ä»¶ img_c:    (B, 4096, 1152)
  
  â†“ Self-Attention
  (åŒä¸Š)
  
  â†“ Cross-Attention (è§†è§‰)
  Q from x:      (B, 16, 35, 72)
  K,V from img:  (B, 16, 4096, 72)
  Attn:          (B, 16, 35, 4096)    â† 35 ä¸ª query, 4096 ä¸ª key
  Output:        (B, 35, 1152)
  
  â†“ FFN
  Output:        (B, 35, 1152)

... (é‡å¤ 28 å±‚)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Final Layer é˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
x:             (B, 35, 1152)
â†“ RmsNorm
               (B, 35, 1152)
â†“ FFN
               (B, 35, 128)           â† æ˜ å°„åˆ°è¾“å‡ºç»´åº¦

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å‡ºé˜¶æ®µ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
x[:, -32:]     (B, 32, 128)          â† åªä¿ç•™åŠ¨ä½œéƒ¨åˆ†
                                      âœ… é¢„æµ‹çš„å»å™ªåŠ¨ä½œåºåˆ—
```

---

## ğŸ’¡ RDT vs DiT å¯¹æ¯”

| ç‰¹æ€§ | DiT | RDT |
|------|-----|-----|
| **ä»»åŠ¡** | å›¾åƒç”Ÿæˆ | æœºå™¨äººæ§åˆ¶ |
| **è¾“å…¥æ•°æ®** | å›¾åƒ | åŠ¨ä½œåºåˆ— + çŠ¶æ€ |
| **æ¡ä»¶ä¿¡æ¯** | æ—¶é—´æ­¥ + ç±»åˆ«æ ‡ç­¾ | æ—¶é—´æ­¥ + é¢‘ç‡ + è¯­è¨€ + è§†è§‰ |
| **Patchify** | âœ… å·ç§¯åˆ‡åˆ†å›¾åƒ | âŒ ç›´æ¥è¾“å…¥ token åºåˆ— |
| **Un-patchify** | âœ… é‡ç»„å›¾åƒ | âŒ ç›´æ¥è¾“å‡ºåºåˆ— |
| **Block ç»“æ„** | Self-Attn + FFN | Self-Attn + Cross-Attn + FFN |
| **æ¡ä»¶æœºåˆ¶** | AdaLN (è°ƒåˆ¶) | Cross-Attention (æ˜¾å¼) |
| **å½’ä¸€åŒ–** | LayerNorm | RmsNorm |
| **å¤šæ¨¡æ€** | âŒ å•æ¨¡æ€ | âœ… è¯­è¨€ + è§†è§‰äº¤æ›¿ |
| **åºåˆ—é•¿åº¦** | å›ºå®š (49 patches) | å¯å˜ (horizon å¯è°ƒ) |
| **æ¨¡å‹è§„æ¨¡** | ~350K | ~1B (RDT-1B) |

---

## ğŸ§ª å…³é”®ä»£ç ç‰‡æ®µ

### **Cross-Attention æ ¸å¿ƒå®ç°**

```python
class CrossAttention:
    def forward(self, x, c, mask=None):
        # x: (B, N, D) åŠ¨ä½œåºåˆ—
        # c: (B, L, D) æ¡ä»¶ (è¯­è¨€æˆ–è§†è§‰)
        # mask: (B, L) æœ‰æ•ˆ token æ©ç 
        
        B, N, D = x.shape
        L = c.shape[1]
        
        # ç”Ÿæˆ Q, K, V
        Q = self.q(x).reshape(B, N, num_heads, head_dim)
                    .permute(0, 2, 1, 3)  # (B, h, N, d)
        
        KV = self.kv(c).reshape(B, L, 2, num_heads, head_dim)
                       .permute(2, 0, 3, 1, 4)  # (2, B, h, L, d)
        K, V = KV[0], KV[1]  # (B, h, L, d)
        
        # åº”ç”¨å½’ä¸€åŒ–
        Q = self.q_norm(Q)
        K = self.k_norm(K)
        
        # æ‰©å±•æ©ç 
        if mask is not None:
            mask = mask.reshape(B, 1, 1, L)
                      .expand(-1, -1, N, -1)  # (B, 1, N, L)
        
        # Flash Attention
        x = F.scaled_dot_product_attention(
            query=Q,     # (B, h, N, d)
            key=K,       # (B, h, L, d)
            value=V,     # (B, h, L, d)
            attn_mask=mask,
            dropout_p=self.attn_drop.p if self.training else 0.
        )  # (B, h, N, d)
        
        # é‡ç»„
        x = x.permute(0, 2, 1, 3).reshape(B, N, D)
        x = self.proj(x)
        return x
```

---

### **RDT Block çš„å®Œæ•´æµç¨‹**

```python
class RDTBlock:
    def forward(self, x, c, mask):
        # 1. Self-Attention (åŠ¨ä½œåºåˆ—å†…éƒ¨äº¤äº’)
        origin_x = x
        x = self.norm1(x)              # RmsNorm
        x = self.attn(x)               # Self-Attention
        x = x + origin_x               # æ®‹å·®è¿æ¥
        
        # 2. Cross-Attention (æ³¨å…¥æ¡ä»¶ä¿¡æ¯)
        origin_x = x
        x = self.norm2(x)              # RmsNorm
        x = self.cross_attn(x, c, mask)  # Cross-Attention
        x = x + origin_x               # æ®‹å·®è¿æ¥
        
        # 3. FFN (éçº¿æ€§å˜æ¢)
        origin_x = x
        x = self.norm3(x)              # RmsNorm
        x = self.ffn(x)                # Feed-Forward
        x = x + origin_x               # æ®‹å·®è¿æ¥
        
        return x
```

---

### **ä¸»æ¨¡å‹å‰å‘ä¼ æ’­**

```python
class RDT:
    def forward(self, x, freq, t, lang_c, img_c, lang_mask, img_mask):
        # 1. æ—¶é—´å’Œé¢‘ç‡ç¼–ç 
        t_emb = self.t_embedder(t).unsqueeze(1)       # (B, 1, D)
        freq_emb = self.freq_embedder(freq).unsqueeze(1)  # (B, 1, D)
        
        # 2. æ‹¼æ¥åˆ°ä¸»åºåˆ—
        if t_emb.shape[0] == 1:
            t_emb = t_emb.expand(x.shape[0], -1, -1)
        x = torch.cat([t_emb, freq_emb, x], dim=1)    # (B, T+2, D)
        
        # 3. æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.x_pos_embed
        lang_c = lang_c + self.lang_cond_pos_embed[:, :lang_c.shape[1]]
        img_c = img_c + self.img_cond_pos_embed
        
        # 4. é€šè¿‡ RDT Blocks (äº¤æ›¿æ¡ä»¶)
        for i, block in enumerate(self.blocks):
            # å¶æ•°å±‚ç”¨è¯­è¨€,å¥‡æ•°å±‚ç”¨è§†è§‰
            c = lang_c if i % 2 == 0 else img_c
            mask = lang_mask if i % 2 == 0 else img_mask
            x = block(x, c, mask)
        
        # 5. è¾“å‡ºå±‚
        x = self.final_layer(x)                       # (B, T+2, out_dim)
        
        # 6. åªä¿ç•™åŠ¨ä½œéƒ¨åˆ†
        x = x[:, -self.horizon:]                      # (B, horizon, out_dim)
        
        return x
```

---

## ğŸ“Š æ¨¡å‹å‚æ•°é‡ä¼°ç®—

**RDT-1B é…ç½®**:
- hidden_size=1152, depth=28, num_heads=16
- horizon=32, output_dim=128

```
Time Embedders (2 ä¸ª):
  - æ¯ä¸ª MLP: 256Ã—1152 + 1152Ã—1152 â‰ˆ 1.6M
  - æ€»è®¡: 3.2M

Position Embeddings:
  - x_pos_embed: 35Ã—1152 = 40K
  - lang_pos_embed: 1024Ã—1152 = 1.2M
  - img_pos_embed: 4096Ã—1152 = 4.7M
  - æ€»è®¡: 6M

RDT Blocks (Ã—28):
  æ¯ä¸ª Block:
    - Self-Attention:
        - QKV Linear: 3Ã—(1152Ã—1152) = 4.0M
        - Proj Linear: 1152Ã—1152 = 1.3M
    - Cross-Attention:
        - Q Linear: 1152Ã—1152 = 1.3M
        - KV Linear: 1152Ã—(2Ã—1152) = 2.7M
        - Proj Linear: 1152Ã—1152 = 1.3M
    - FFN:
        - FC1: 1152Ã—1152 = 1.3M
        - FC2: 1152Ã—1152 = 1.3M
    - RmsNorm (3 ä¸ª): å¿½ç•¥ä¸è®¡
    
    æ¯ä¸ª Block â‰ˆ 13.2M
    28 ä¸ª Block â‰ˆ 370M

Final Layer:
  - FFN: 1152Ã—1152 + 1152Ã—128 â‰ˆ 1.5M

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»è®¡: ~380M å‚æ•° (ä¸å«å¤šæ¨¡æ€ç¼–ç å™¨)

å¦‚æœåŠ ä¸Š:
  - è¯­è¨€ç¼–ç å™¨ (T5-XL): ~3B å‚æ•°
  - è§†è§‰ç¼–ç å™¨ (ResNet-50): ~25M å‚æ•°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å®Œæ•´ RDT-1B: ~3.4B å‚æ•°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ å­¦ä¹ å»ºè®®

### **ç†è§£é¡ºåº**

1. **ç¬¬ä¸€æ­¥**: ç†è§£ `TimestepEmbedder`ï¼ˆæ—¶é—´ç¼–ç ,æœ€ç®€å•ï¼‰
2. **ç¬¬äºŒæ­¥**: ç†è§£ `CrossAttention`ï¼ˆæ ¸å¿ƒæœºåˆ¶ï¼‰
3. **ç¬¬ä¸‰æ­¥**: ç†è§£ `RDTBlock`ï¼ˆå¦‚ä½•ç»„åˆ Self-Attn + Cross-Attnï¼‰
4. **ç¬¬å››æ­¥**: ç†è§£ `RDT` ä¸»æ¨¡å‹ï¼ˆäº¤æ›¿æ¡ä»¶çš„è®¾è®¡ï¼‰
5. **ç¬¬äº”æ­¥**: ç†è§£è®­ç»ƒè„šæœ¬ï¼ˆå¤šæ¨¡æ€æ•°æ®åŠ è½½ï¼‰

### **è°ƒè¯•æŠ€å·§**

åœ¨å…³é”®ä½ç½®æ·»åŠ  shape æ‰“å°:
```python
print(f"After time embedding: {t_emb.shape}")
print(f"After concat: {x.shape}")
print(f"Lang condition: {lang_c.shape}, mask: {lang_mask.shape}")
print(f"After block {i}: {x.shape}")
print(f"Final output: {x.shape}")
```

### **åŠ¨æ‰‹å®éªŒ**

1. **ä¿®æ”¹ horizon**: ä» 32 æ”¹åˆ° 16,è§‚å¯Ÿ shape å˜åŒ–
2. **å¯è§†åŒ–æ³¨æ„åŠ›**: æ‰“å° Cross-Attention çš„ attention map,çœ‹æ¨¡å‹å…³æ³¨å“ªäº›è¯­è¨€/è§†è§‰ token
3. **æ¶ˆèå®éªŒ**: åªç”¨è¯­è¨€æ¡ä»¶æˆ–åªç”¨è§†è§‰æ¡ä»¶,å¯¹æ¯”æ€§èƒ½
4. **æ¡ä»¶æ©ç **: å®éªŒä¸åŒçš„ mask ç­–ç•¥

### **å¸¸è§é—®é¢˜**

**Q1: ä¸ºä»€ä¹ˆäº¤æ›¿ä½¿ç”¨è¯­è¨€å’Œè§†è§‰æ¡ä»¶?**
- A: å……åˆ†èåˆä¸¤ç§ä¿¡æ¯,æ¯å±‚éƒ½èƒ½è®¿é—®ä¸€ç§æ¡ä»¶,é¿å…ä¿¡æ¯ç“¶é¢ˆ

**Q2: RmsNorm vs LayerNorm æœ‰ä»€ä¹ˆåŒºåˆ«?**
- A: RmsNorm ä¸å‡å»å‡å€¼,åªåšå°ºåº¦å½’ä¸€åŒ–,è®­ç»ƒæ›´ç¨³å®š,é€Ÿåº¦æ›´å¿«

**Q3: ä¸ºä»€ä¹ˆæœ€ååªä¿ç•™åŠ¨ä½œéƒ¨åˆ†?**
- A: è¾“å…¥çš„ timestep, freq, state åªæ˜¯è¾…åŠ©ä¿¡æ¯,æˆ‘ä»¬åªéœ€è¦é¢„æµ‹/å»å™ªåŠ¨ä½œåºåˆ—

**Q4: Flash Attention æ˜¯ä»€ä¹ˆ?**
- A: é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°,å‡å°‘æ˜¾å­˜å ç”¨,åŠ é€Ÿè®¡ç®—,PyTorch 2.0+ å†…ç½®

---

## ğŸ”— å‚è€ƒèµ„æ–™

- **RDT è®ºæ–‡**: [RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation](https://arxiv.org/abs/2410.07864)
- **DiT è®ºæ–‡**: [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)
- **DDPM**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
- **Flash Attention**: [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)

---

## ğŸ“Œ å…³é”®æœ¯è¯­å¯¹ç…§è¡¨

| è‹±æ–‡ | ä¸­æ–‡ | è¯´æ˜ |
|------|------|------|
| Horizon | é¢„æµ‹æ­¥æ•° | åŠ¨ä½œåºåˆ—çš„é•¿åº¦ (å¦‚ 32 æ­¥) |
| Control Frequency | æ§åˆ¶é¢‘ç‡ | æœºå™¨äººæ‰§è¡ŒåŠ¨ä½œçš„é¢‘ç‡ (å¦‚ 10Hz) |
| Cross-Attention | è·¨æ³¨æ„åŠ› | Query å’Œ Key æ¥è‡ªä¸åŒåºåˆ— |
| Self-Attention | è‡ªæ³¨æ„åŠ› | Query å’Œ Key æ¥è‡ªåŒä¸€åºåˆ— |
| RmsNorm | å‡æ–¹æ ¹å½’ä¸€åŒ– | Root Mean Square Layer Normalization |
| EMA | æŒ‡æ•°ç§»åŠ¨å¹³å‡ | Exponential Moving Average |
| Multimodal | å¤šæ¨¡æ€ | è¯­è¨€ + è§†è§‰ + å…¶ä»–æ¨¡æ€ |
| Token | ä»¤ç‰Œ | åºåˆ—ä¸­çš„åŸºæœ¬å•ä½ |
| Embedding | åµŒå…¥ | å°†ç¦»æ•£ç¬¦å·æ˜ å°„åˆ°è¿ç»­å‘é‡ |

---

**ç¥ä½ å­¦ä¹ é¡ºåˆ©!æœºå™¨äººæ§åˆ¶å’Œå¤šæ¨¡æ€å­¦ä¹ æ˜¯æœªæ¥çš„é‡è¦æ–¹å‘** ğŸ¤–ğŸ‰
