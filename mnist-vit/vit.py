from torch import nn 
import torch 
import pdb  # è°ƒè¯•å·¥å…·
import os

# ğŸ›ï¸ è°ƒè¯•å¼€å…³ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ DEBUG_INIT=1 æ¥å¯ç”¨ __init__ ä¸­çš„æ–­ç‚¹
DEBUG_INIT = os.getenv('DEBUG_INIT', '0') == '1'

class ViT(nn.Module):
    print("è¿›å…¥ ViT ç±»å®šä¹‰")
    def __init__(self,emb_size=16):
        super().__init__()
        
        # ğŸ” æ–­ç‚¹1ï¼šæŸ¥çœ‹åˆå§‹åŒ–å‚æ•°
        if DEBUG_INIT: pdb.set_trace()
        
        self.patch_size=4
        self.patch_count=28//self.patch_size # 7
        
        # ğŸ” æ–­ç‚¹2ï¼šç†è§£å·ç§¯å±‚å¦‚ä½•åˆ‡åˆ†patch
        # Conv2d: æŠŠ28x28å›¾åƒåˆ‡æˆ7x7ä¸ªpatchï¼Œæ¯ä¸ªpatchæ˜¯4x4=16ä¸ªåƒç´ 
        if DEBUG_INIT: pdb.set_trace()
        self.conv=nn.Conv2d(in_channels=1,out_channels=self.patch_size**2,kernel_size=self.patch_size,padding=0,stride=self.patch_size) # å›¾ç‰‡è½¬patch
        
        # ğŸ” æ–­ç‚¹3ï¼šç†è§£patch embedding
        # Linear: æŠŠ16ç»´çš„patchå±•å¹³å‘é‡æ˜ å°„åˆ°emb_sizeç»´
        if DEBUG_INIT: pdb.set_trace()
        self.patch_emb=nn.Linear(in_features=self.patch_size**2,out_features=emb_size)    # patchåšemb
        
        # ğŸ” æ–­ç‚¹4ï¼šç†è§£CLS tokenå’Œä½ç½®ç¼–ç 
        if DEBUG_INIT: pdb.set_trace()
        self.cls_token=nn.Parameter(torch.rand(1,1,emb_size))   # åˆ†ç±»å¤´è¾“å…¥
        self.pos_emb=nn.Parameter(torch.rand(1,self.patch_count**2+1,emb_size))   # positionä½ç½®å‘é‡ (1,seq_len,emb_size)
        
        # ğŸ” æ–­ç‚¹5ï¼šç†è§£Transformer Encoderç»“æ„
        if DEBUG_INIT: pdb.set_trace()
        #encoder ç°è‰²éƒ¨åˆ†å †ä¸‰ä¸ª
        self.tranformer_enc=nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=emb_size,nhead=2,batch_first=True),num_layers=3)   # transformerç¼–ç å™¨
        self.cls_linear=nn.Linear(in_features=emb_size,out_features=10) # æ‰‹å†™æ•°å­—10åˆ†ç±»
        
    def forward(self,x): # (batch_size,channel=1,width=28,height=28)
        print("è¿›å…¥ ViT å‰å‘ä¼ æ’­")
        # ğŸ” æ–­ç‚¹6ï¼šæŸ¥çœ‹è¾“å…¥å›¾åƒshape
        print(f"è¾“å…¥ x.shape: {x.shape}")  # åº”è¯¥æ˜¯ (batch, 1, 28, 28)
        pdb.set_trace()
        print("9.1")
        # æ­¥éª¤1: å·ç§¯åˆ‡åˆ†æˆpatch
        x=self.conv(x) # (batch_size,channel=16,width=7,height=7)
        
        # ğŸ” æ–­ç‚¹7ï¼šæŸ¥çœ‹å·ç§¯åçš„patch
        print(f"å·ç§¯å x.shape: {x.shape}")  # åº”è¯¥æ˜¯ (batch, 16, 7, 7)
        pdb.set_trace()
        print("9.2")
        # æ­¥éª¤2: é‡å¡‘ä¸ºåºåˆ—å½¢å¼
        x=x.view(x.size(0),x.size(1),self.patch_count**2)   # (batch_size,channel=16,seq_len=49)
        
        # ğŸ” æ–­ç‚¹8ï¼šæŸ¥çœ‹viewåçš„shape
        print(f"viewå x.shape: {x.shape}")  # åº”è¯¥æ˜¯ (batch, 16, 49)
        pdb.set_trace()
        print("9.3")
        x=x.permute(0,2,1)  # (batch_size,seq_len=49,channel=16)
        
        # ğŸ” æ–­ç‚¹9ï¼šæŸ¥çœ‹permuteåï¼Œåºåˆ—å½¢å¼
        print(f"permuteå x.shape: {x.shape}")  # åº”è¯¥æ˜¯ (batch, 49, 16)
        pdb.set_trace()
        print("9.4")
        # æ­¥éª¤3: patch embedding
        x=self.patch_emb(x)   # (batch_size,seq_len=49,emb_size)
        
        # ğŸ” æ–­ç‚¹10ï¼šæŸ¥çœ‹embeddingåçš„ç‰¹å¾
        print(f"patch_embå x.shape: {x.shape}")  # åº”è¯¥æ˜¯ (batch, 49, emb_size)
        pdb.set_trace()
        print("9.5")
        # æ­¥éª¤4: æ·»åŠ CLS token
        cls_token=self.cls_token.expand(x.size(0),1,x.size(2))  # (batch_size,1,emb_size)
        
        # ğŸ” æ–­ç‚¹11ï¼šæŸ¥çœ‹CLS token
        print(f"cls_token.shape: {cls_token.shape}")  # åº”è¯¥æ˜¯ (batch, 1, emb_size)
        pdb.set_trace()
        print("9.6")
        x=torch.cat((cls_token,x),dim=1)   # add [cls] token
        
        # ğŸ” æ–­ç‚¹12ï¼šæŸ¥çœ‹æ‹¼æ¥CLSåçš„åºåˆ—
        print(f"æ‹¼æ¥CLSå x.shape: {x.shape}")  # åº”è¯¥æ˜¯ (batch, 50, emb_size)
        pdb.set_trace()
        print("9.7")
        # æ­¥éª¤5: æ·»åŠ ä½ç½®ç¼–ç 
        x=self.pos_emb+x
        
        # ğŸ” æ–­ç‚¹13ï¼šæŸ¥çœ‹åŠ ä½ç½®ç¼–ç å
        print(f"åŠ ä½ç½®ç¼–ç å x.shape: {x.shape}")  # åº”è¯¥æ˜¯ (batch, 50, emb_size)
        pdb.set_trace()
        print("9.8")
        # æ­¥éª¤6: Transformerç¼–ç 
        y=self.tranformer_enc(x) # ä¸æ¶‰åŠpaddingï¼Œæ‰€ä»¥ä¸éœ€è¦mask
        
        # ğŸ” æ–­ç‚¹14ï¼šæŸ¥çœ‹Transformerè¾“å‡º
        print(f"Transformerå y.shape: {y.shape}")  # åº”è¯¥æ˜¯ (batch, 50, emb_size)
        pdb.set_trace()
        print("9.9")
        # æ­¥éª¤7: å–CLS tokenè¾“å‡ºåšåˆ†ç±»
        cls_output = y[:,0,:]  # å–ç¬¬ä¸€ä¸ªtoken (CLS)
        
        # ğŸ” æ–­ç‚¹15ï¼šæŸ¥çœ‹CLSè¾“å‡º
        print(f"CLSè¾“å‡º cls_output.shape: {cls_output.shape}")  # åº”è¯¥æ˜¯ (batch, emb_size)
        pdb.set_trace()
        print("9.10")
        logits = self.cls_linear(cls_output)
        
        # ğŸ” æ–­ç‚¹16ï¼šæŸ¥çœ‹æœ€ç»ˆåˆ†ç±»logits
        print(f"æœ€ç»ˆlogits.shape: {logits.shape}")  # åº”è¯¥æ˜¯ (batch, 10)
        print(f"é¢„æµ‹ç±»åˆ«: {logits.argmax(-1)}")
        pdb.set_trace()
        
        return logits   # å¯¹[CLS] tokenè¾“å‡ºåšåˆ†ç±»
    
if __name__=='__main__':
    vit=ViT()
    print("vit")
    x=torch.rand(5,1,28,28)
    y=vit(x)
    print(y.shape)